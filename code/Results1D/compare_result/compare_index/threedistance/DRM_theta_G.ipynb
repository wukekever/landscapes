{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from math import *\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defination of activation function\n",
    "def activation(x):\n",
    "    return x * torch.sigmoid(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ResNet with one blocks\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,width):\n",
    "        super(Net,self).__init__()\n",
    "        self.layer_in = nn.Linear(input_size,width)\n",
    "        self.layer_1 = nn.Linear(width,width)\n",
    "        self.layer_2 = nn.Linear(width,width)\n",
    "        self.layer_out = nn.Linear(width,1)\n",
    "    def forward(self,x):\n",
    "        output = self.layer_in(x)\n",
    "        output = output + activation(self.layer_2(activation(self.layer_1(output)))) # residual block 1\n",
    "        output = self.layer_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "width = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact solution\n",
    "def u_ex(x):  \n",
    "    return torch.sin(pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x)\n",
    "def f(x):\n",
    "    return pi**2 * torch.sin(pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_num = 200\n",
    "x = torch.zeros(grid_num + 1, input_size)\n",
    "for index in range(grid_num + 1):\n",
    "    x[index] = index * 1 / grid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x * (x - 1.0) * net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function to DRM by auto differential\n",
    "def loss_function(x):\n",
    "    h = 1 / grid_num\n",
    "    sum_0 = 0.0\n",
    "    sum_1 = 0.0\n",
    "    sum_2 = 0.0\n",
    "    sum_a = 0.0\n",
    "    sum_b = 0.0\n",
    "    for index in range(grid_num):\n",
    "        x_temp = x[index] + h / 2 \n",
    "        x_temp.requires_grad = True\n",
    "        grad_x_temp = torch.autograd.grad(outputs = model(x_temp), inputs = x_temp, grad_outputs = torch.ones(model(x_temp).shape), create_graph = True)\n",
    "        sum_1 += (0.5*grad_x_temp[0]**2 - f(x_temp)[0]*model(x_temp)[0])\n",
    "        \n",
    "    for index in range(1, grid_num):\n",
    "        x_temp = x[index]\n",
    "        x_temp.requires_grad = True\n",
    "        grad_x_temp = torch.autograd.grad(outputs = model(x_temp), inputs = x_temp, grad_outputs = torch.ones(model(x_temp).shape), create_graph = True)\n",
    "        sum_2 += (0.5*grad_x_temp[0]**2 - f(x_temp)[0]*model(x_temp)[0])\n",
    "    \n",
    "    x_temp = x[0]\n",
    "    x_temp.requires_grad = True\n",
    "    grad_x_temp = torch.autograd.grad(outputs = model(x_temp), inputs = x_temp, grad_outputs = torch.ones(model(x_temp).shape), create_graph = True)\n",
    "    sum_a = 0.5*grad_x_temp[0]**2 - f(x_temp)[0]*model(x_temp)[0]\n",
    "    \n",
    "    x_temp = x[grid_num]\n",
    "    x_temp.requires_grad = True\n",
    "    grad_x_temp = torch.autograd.grad(outputs = model(x_temp), inputs = x_temp, grad_outputs = torch.ones(model(x_temp).shape), create_graph = True)\n",
    "    sum_a = 0.5*grad_x_temp[0]**2 - f(x_temp)[0]*model(x_temp)[0]\n",
    "    \n",
    "    sum_0 = h / 6 * (sum_a + 4 * sum_1 + 2 * sum_2 + sum_b)\n",
    "    return sum_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model parameters on cpu\n",
    "pretrained_dict = torch.load('net_params_DGM.pkl', map_location = 'cpu')\n",
    "    \n",
    "# get state_dict\n",
    "net_state_dict = net.state_dict()\n",
    "\n",
    "# remove keys that does not belong to net_state_dict\n",
    "pretrained_dict_1 = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "\n",
    "# update dict\n",
    "net_state_dict.update(pretrained_dict_1)\n",
    "\n",
    "# set new dict back to net\n",
    "net.load_state_dict(net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(net):\n",
    "    \"\"\" Extract parameters from net, and return a list of tensors\"\"\"\n",
    "    return [p.data for p in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weights(net, weights, directions=None, step=None):\n",
    "    \"\"\"\n",
    "        Overwrite the network's weights with a specified list of tensors\n",
    "        or change weights along directions with a step size.\n",
    "    \"\"\"\n",
    "    if directions is None:\n",
    "        # You cannot specify a step length without a direction.\n",
    "        for (p, w) in zip(net.parameters(), weights):\n",
    "            p.data.copy_(w.type(type(p.data)))\n",
    "    else:\n",
    "        assert step is not None, 'If a direction is specified then step must be specified as well'\n",
    "\n",
    "        if len(directions) == 2:\n",
    "            dx = directions[0]\n",
    "            dy = directions[1]\n",
    "            changes = [d0*step[0] + d1*step[1] for (d0, d1) in zip(dx, dy)]\n",
    "        else:\n",
    "            changes = [d*step for d in directions[0]]\n",
    "\n",
    "        for (p, w, d) in zip(net.parameters(), weights, changes):\n",
    "            p.data = w + torch.Tensor(d).type(type(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_states(net, states, directions=None, step=None):\n",
    "    \"\"\"\n",
    "        Overwrite the network's state_dict or change it along directions with a step size.\n",
    "    \"\"\"\n",
    "    if directions is None:\n",
    "        net.load_state_dict(states)\n",
    "    else:\n",
    "        assert step is not None, 'If direction is provided then the step must be specified as well'\n",
    "        if len(directions) == 2:\n",
    "            dx = directions[0]\n",
    "            dy = directions[1]\n",
    "            changes = [d0*step[0] + d1*step[1] for (d0, d1) in zip(dx, dy)]\n",
    "        else:\n",
    "            changes = [d*step for d in directions[0]]\n",
    "\n",
    "        new_states = copy.deepcopy(states)\n",
    "        assert (len(new_states) == len(changes))\n",
    "        for (k, v), d in zip(new_states.items(), changes):\n",
    "            d = torch.tensor(d)\n",
    "            v.add_(d.type(v.type()))\n",
    "\n",
    "        net.load_state_dict(new_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_weights(weights):\n",
    "    \"\"\"\n",
    "        Produce a random direction that is a list of random Gaussian tensors\n",
    "        with the same shape as the network's weights, so one direction entry per weight.\n",
    "    \"\"\"\n",
    "    return [torch.randn(w.size()) for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_states(states):\n",
    "    \"\"\"\n",
    "        Produce a random direction that is a list of random Gaussian tensors\n",
    "        with the same shape as the network's state_dict(), so one direction entry\n",
    "        per weight, including BN's running_mean/var.\n",
    "    \"\"\"\n",
    "    return [torch.randn(w.size()) for k, w in states.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_weights(weights, weights2):\n",
    "    \"\"\" Produce a direction from 'weights' to 'weights2'.\"\"\"\n",
    "    return [w2 - w for (w, w2) in zip(weights, weights2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_states(states, states2):\n",
    "    \"\"\" Produce a direction from 'states' to 'states2'.\"\"\"\n",
    "    return [v2 - v for (k, v), (k2, v2) in zip(states.items(), states2.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_direction(direction, weights, norm='filter'):\n",
    "    \"\"\"\n",
    "        Rescale the direction so that it has similar norm as their corresponding\n",
    "        model in different levels.\n",
    "        Args:\n",
    "          direction: a variables of the random direction for one layer\n",
    "          weights: a variable of the original model for one layer\n",
    "          norm: normalization method, 'filter' | 'layer' | 'weight'\n",
    "    \"\"\"\n",
    "    if norm == 'filter':\n",
    "        # Rescale the filters (weights in group) in 'direction' so that each\n",
    "        # filter has the same norm as its corresponding filter in 'weights'.\n",
    "        for d, w in zip(direction, weights):\n",
    "            d.mul_(w.norm()/(d.norm() + 1e-10))\n",
    "    elif norm == 'layer':\n",
    "        # Rescale the layer variables in the direction so that each layer has\n",
    "        # the same norm as the layer variables in weights.\n",
    "        direction.mul_(weights.norm()/direction.norm())\n",
    "    elif norm == 'weight':\n",
    "        # Rescale the entries in the direction so that each entry has the same\n",
    "        # scale as the corresponding weight.\n",
    "        direction.mul_(weights)\n",
    "    elif norm == 'dfilter':\n",
    "        # Rescale the entries in the direction so that each filter direction\n",
    "        # has the unit norm.\n",
    "        for d in direction:\n",
    "            d.div_(d.norm() + 1e-10)\n",
    "    elif norm == 'dlayer':\n",
    "        # Rescale the entries in the direction so that each layer direction has\n",
    "        # the unit norm.\n",
    "        direction.div_(direction.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_directions_for_weights(direction, weights, norm='filter', ignore='biasbn'):\n",
    "    \"\"\"\n",
    "        The normalization scales the direction entries according to the entries of weights.\n",
    "    \"\"\"\n",
    "    assert(len(direction) == len(weights))\n",
    "    for d, w in zip(direction, weights):\n",
    "        if d.dim() <= 1:\n",
    "            if ignore == 'biasbn':\n",
    "                d.fill_(0) # ignore directions for weights with 1 dimension\n",
    "            else:\n",
    "                d.copy_(w) # keep directions for weights/bias that are only 1 per node\n",
    "        else:\n",
    "            normalize_direction(d, w, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_directions_for_states(direction, states, norm='filter', ignore='ignore'):\n",
    "    assert(len(direction) == len(states))\n",
    "    for d, (k, w) in zip(direction, states.items()):\n",
    "        if d.dim() <= 1:\n",
    "            if ignore == 'biasbn':\n",
    "                d.fill_(0) # ignore directions for weights with 1 dimension\n",
    "            else:\n",
    "                d.copy_(w) # keep directions for weights/bias that are only 1 per node\n",
    "        else:\n",
    "            normalize_direction(d, w, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_biasbn(directions):\n",
    "    \"\"\" Set bias and bn parameters in directions to zero \"\"\"\n",
    "    for d in directions:\n",
    "        if d.dim() <= 1:\n",
    "            d.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_direction(net, dir_type='weights', ignore='biasbn', norm='filter'):\n",
    "    \"\"\"\n",
    "        Setup a random (normalized) direction with the same dimension as\n",
    "        the weights or states.\n",
    "        Args:\n",
    "          net: the given trained model\n",
    "          dir_type: 'weights' or 'states', type of directions.\n",
    "          ignore: 'biasbn', ignore biases and BN parameters.\n",
    "          norm: direction normalization method, including\n",
    "                'filter\" | 'layer' | 'weight' | 'dlayer' | 'dfilter'\n",
    "        Returns:\n",
    "          direction: a random direction with the same dimension as weights or states.\n",
    "    \"\"\"\n",
    "\n",
    "    # random direction\n",
    "    if dir_type == 'weights':\n",
    "        weights = get_weights(net) # a list of parameters.\n",
    "        direction = get_random_weights(weights)\n",
    "        normalize_directions_for_weights(direction, weights, norm, ignore)\n",
    "    elif dir_type == 'states':\n",
    "        states = net.state_dict() # a dict of parameters, including BN's running mean/var.\n",
    "        direction = get_random_states(states)\n",
    "        normalize_directions_for_states(direction, states, norm, ignore)\n",
    "\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_temp = get_weights(net)\n",
    "states_temp = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.0005  \n",
    "grid = np.arange(-0.01, 0.01 + step_size, step_size)\n",
    "loss_matrix = np.zeros((len(grid), len(grid)))\n",
    "time_start = time.time()\n",
    "for dx in grid:\n",
    "    itemindex = np.argwhere(grid == dx)\n",
    "    print('[Finished: {:}/{:}]'.format(itemindex[0][0] + 1, len(grid)))\n",
    "    for dy in grid:\n",
    "        itemindex_1 = np.argwhere(grid == dx)\n",
    "        itemindex_2 = np.argwhere(grid == dy)\n",
    "        \n",
    "        weights = weights_temp\n",
    "        states = states_temp \n",
    "        step = [dx, dy]\n",
    "        direction_1 = create_random_direction(net, dir_type='weights', ignore='biasbn', norm='filter')\n",
    "        normalize_directions_for_states(direction_1, states, norm='filter', ignore='ignore')\n",
    "        direction_2 = create_random_direction(net, dir_type='weights', ignore='biasbn', norm='filter')\n",
    "        normalize_directions_for_states(direction_2, states, norm='filter', ignore='ignore')\n",
    "        directions = [direction_1, direction_2]\n",
    "        set_states(net, states, directions, step)\n",
    "        loss_temp = loss_function(x)\n",
    "        loss_matrix[itemindex_1[0][0], itemindex_2[0][0]] = loss_temp\n",
    "        \n",
    "        # get state_dict\n",
    "        net_state_dict = net.state_dict()\n",
    "        # remove keys that does not belong to net_state_dict\n",
    "        pretrained_dict_1 = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "        # update dict\n",
    "        net_state_dict.update(pretrained_dict_1)\n",
    "        # set new dict back to net\n",
    "        net.load_state_dict(net_state_dict)\n",
    "        weights_temp = get_weights(net)\n",
    "        states_temp = net.state_dict()\n",
    "time_end = time.time()\n",
    "print('total time is: ', time_end-time_start, 'seconds')               \n",
    "np.save(\"loss_matrix_DRM_at_theta_G.npy\",loss_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
